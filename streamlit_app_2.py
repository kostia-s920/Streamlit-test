import tempfile
import base64
import plotly.graph_objects as go
import re
import streamlit as st
import psycopg2
import pandas as pd
from datetime import datetime
import difflib
import streamlit.components.v1 as components
import requests
from bs4 import BeautifulSoup
import json
import logging

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö PostgreSQL
def connect_to_db():
    try:
        # –î–µ–∫–æ–¥—É—î–º–æ —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç —ñ–∑ Base64
        ssl_cert_decoded = base64.b64decode(st.secrets["db_ssl_root_cert"])

        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç–∞
        with tempfile.NamedTemporaryFile(delete=False) as cert_file:
            cert_file.write(ssl_cert_decoded)
            cert_file_path = cert_file.name

        # –ü—ñ–¥–∫–ª—é—á–∞—î–º–æ—Å—è –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Å–µ–∫—Ä–µ—Ç—ñ–≤
        connection = psycopg2.connect(
            host=st.secrets["db_host"],
            database=st.secrets["db_name"],
            user=st.secrets["db_username"],
            password=st.secrets["db_password"],
            port=st.secrets["db_port"],
            sslmode=st.secrets["ssl_mode"],
            sslrootcert=cert_file_path  # –ü–µ—Ä–µ–¥–∞—î–º–æ —à–ª—è—Ö –¥–æ —Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É –∑ —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç–æ–º
        )
        return connection
    except Exception as e:
        st.error(f"Error connecting to database: {e}")
        return None

#–§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤
def get_competitors_from_content_changes(conn):
    query = "SELECT DISTINCT competitor_name FROM content_changes"
    return pd.read_sql(query, conn)['competitor_name'].tolist()


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É URL –¥–ª—è –æ–±—Ä–∞–Ω–æ–≥–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞
def get_pages_for_competitor(conn, competitor_name):
    query = f"SELECT DISTINCT url FROM {competitor_name}"
    return pd.read_sql(query, conn)['url'].tolist()


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞—Ç –¥–ª—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏
def get_dates_for_page(conn, competitor_name, page_url):
    query = f"""
        SELECT DISTINCT date_checked
        FROM {competitor_name}
        WHERE url = '{page_url}'
        ORDER BY date_checked ASC
    """
    return pd.read_sql(query, conn)['date_checked'].tolist()


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–∞ –æ–±—Ä–∞–Ω—É –¥–∞—Ç—É
def get_page_data(conn, competitor_name, page_url, date):
    query = f"""
        SELECT title, h1, description, content, keywords_found, keywords_count
        FROM {competitor_name}
        WHERE url = %s AND date_checked = %s
    """
    return pd.read_sql(query, conn, params=[page_url, date])


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ Plotly —Ç–∞–±–ª–∏—Ü—ñ –¥–ª—è –∑–º—ñ–Ω —É –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
def visualize_metadata_changes(metadata_changes):
    fig = go.Figure(data=[go.Table(
        header=dict(
            values=["–ü–æ–ª–µ", "–ë—É–ª–æ", "–°—Ç–∞–ª–æ"],
            fill_color='paleturquoise',
            align='left',
            font=dict(color='black')  # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –∫–æ–ª—ñ—Ä —Ç–µ–∫—Å—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        ),
        cells=dict(
            values=[metadata_changes['–ü–æ–ª–µ'], metadata_changes['–ë—É–ª–æ'], metadata_changes['–°—Ç–∞–ª–æ']],
            fill_color='lavender',
            align='left',
            font=dict(color='black')  # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –∫–æ–ª—ñ—Ä —Ç–µ–∫—Å—Ç—É –≤ –∫–ª—ñ—Ç–∏–Ω–∫–∞—Ö
        ))
    ])
    fig.update_layout(width=800, height=400)
    st.plotly_chart(fig)


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ Plotly —Ç–∞–±–ª–∏—Ü—ñ –¥–ª—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
def visualize_keywords_changes(keywords_changes):
    fig = go.Figure(data=[go.Table(
        header=dict(
            values=["–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ", "–ó–º—ñ–Ω–∞", "–ë—É–ª–æ", "–°—Ç–∞–ª–æ"],
            fill_color='paleturquoise',
            align='left',
            font=dict(color='black')  # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ —á–æ—Ä–Ω–∏–π –∫–æ–ª—ñ—Ä —Ç–µ–∫—Å—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        ),
        cells=dict(
            values=[
                keywords_changes['–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ'],
                keywords_changes['–ó–º—ñ–Ω–∞'],
                keywords_changes['–ë—É–ª–æ'],
                keywords_changes['–°—Ç–∞–ª–æ']
            ],
            fill_color='lavender',
            align='left',
            font=dict(color='black')  # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ —á–æ—Ä–Ω–∏–π –∫–æ–ª—ñ—Ä —Ç–µ–∫—Å—Ç—É –≤ –∫–ª—ñ—Ç–∏–Ω–∫–∞—Ö
        ))
    ])
    fig.update_layout(width=800, height=400)
    st.plotly_chart(fig)


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É —ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –∑–º—ñ–Ω –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Plotly
def visualize_content_changes(content_before, content_after):

    # –†–æ–∑–±–∏–≤–∞—î–º–æ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Ä—è–¥–∫–∏
    before_lines = content_before.splitlines()
    after_lines = content_after.splitlines()

    # –°—Ç–≤–æ—Ä—é—î–º–æ –æ–±'—î–∫—Ç HtmlDiff
    differ = difflib.HtmlDiff(wrapcolumn=50)

    # –ì–µ–Ω–µ—Ä—É—î–º–æ HTML –∑ –ø—ñ–¥—Å–≤—ñ—á–µ–Ω–∏–º–∏ –∑–º—ñ–Ω–∞–º–∏
    diff_html = differ.make_file(before_lines, after_lines, fromdesc='–ë—É–ª–æ', todesc='–°—Ç–∞–ª–æ')

    # –í—ñ–¥–æ–±—Ä–∞–∂–∞—î–º–æ HTML —É Streamlit
    st.subheader("–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É:")
    components.html(diff_html, height=600,scrolling=True)


# –û–Ω–æ–≤–ª–µ–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏–ª—É—á–µ–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ —ñ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —ó—Ö –ø–æ–≤—Ç–æ—Ä–µ–Ω—å
def extract_keywords(row):
    if pd.isna(row) or not row.strip():
        return {}
    entries = row.split(',')
    keywords_dict = {}
    for entry in entries:
        entry = entry.strip()
        match = re.match(r'^\s*(?P<keyword>.*?)\s*-\s*(?P<count>\d+)\s*—Ä–∞–∑—ñ–≤', entry)
        if match:
            keyword = match.group('keyword').strip().lower()
            count = int(match.group('count'))
            if keyword in keywords_dict:
                keywords_dict[keyword] += count
            else:
                keywords_dict[keyword] = count
    return keywords_dict


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è keywords_found
def compare_keywords(old_keywords, new_keywords):
    old_dict = extract_keywords(old_keywords)
    new_dict = extract_keywords(new_keywords)

    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –¥–æ–¥–∞–Ω—ñ, –≤–∏–¥–∞–ª–µ–Ω—ñ —ñ –∑–º—ñ–Ω–µ–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
    added = {k: new_dict[k] for k in new_dict if k not in old_dict}
    removed = {k: old_dict[k] for k in old_dict if k not in new_dict}
    changed = {k: (old_dict[k], new_dict[k]) for k in old_dict if k in new_dict and old_dict[k] != new_dict[k]}

    result = []
    for k, v in added.items():
        result.append((k, '–î–æ–¥–∞–Ω–æ', '-', f"{v} —Ä–∞–∑—ñ–≤"))
    for k, v in removed.items():
        result.append((k, '–í–∏–¥–∞–ª–µ–Ω–æ', f"{v} —Ä–∞–∑—ñ–≤", '-'))
    for k, (old_v, new_v) in changed.items():
        result.append((k, '–ó–º—ñ–Ω–µ–Ω–æ', f"{old_v} —Ä–∞–∑—ñ–≤", f"{new_v} —Ä–∞–∑—ñ–≤"))

    return pd.DataFrame(result, columns=['–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ', '–ó–º—ñ–Ω–∞', '–ë—É–ª–æ', '–°—Ç–∞–ª–æ'])


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –ø–æ –∫–ª—é—á–æ–≤–∏–º —Å–ª–æ–≤–∞–º —ñ–∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
def get_keyword_data(conn, competitor_name):
    query = f"""
        SELECT url, keywords_count, keywords_found, content, date_checked 
        FROM {competitor_name}
        ORDER BY date_checked ASC
    """
    df = pd.read_sql(query, conn)
    return df

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö –ø–æ –≤–∏–±—Ä–∞–Ω–æ–º—É –∫–ª—é—á–æ–≤–æ–º—É —Å–ª–æ–≤—É
def get_keyword_history(conn, competitor_name, keyword):
    query = f"""
        SELECT url, date_checked, keywords_found
        FROM {competitor_name}
        WHERE keywords_found ILIKE %s
        ORDER BY date_checked ASC
    """
    df = pd.read_sql(query, conn, params=[f'%{keyword}%'])
    return df

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –≥—Ä–∞—Ñ—ñ–∫–∞ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
def plot_keyword_trend(df, competitor_name):
    # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è 'date_checked' —É —Ñ–æ—Ä–º–∞—Ç datetime
    df['date_checked'] = pd.to_datetime(df['date_checked'])

    fig = go.Figure()

    # –ü—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –≤—Å—ñ–º —É–Ω—ñ–∫–∞–ª—å–Ω–∏–º URL
    for url in df['url'].unique():
        url_data = df[df['url'] == url]
        fig.add_trace(go.Scatter(
            x=url_data['date_checked'],
            y=url_data['keywords_count'],
            mode='lines+markers',
            name=url,
            hoverinfo='text',
            hovertext=[
                f"–î–∞—Ç–∞: {date.strftime('%Y-%m-%d')}<br>–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤: {count}"
                for date, count in zip(url_data['date_checked'], url_data['keywords_count'])
            ]
        ))

    # –î–æ–¥–∞—î–º–æ –ø–æ—è—Å–Ω–µ–Ω–Ω—è –¥–æ –≥—Ä–∞—Ñ—ñ–∫–∞
    fig.update_layout(
        title={
            'text': f'–¢—Ä–µ–Ω–¥ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –¥–ª—è {competitor_name}',
            'y':0.9,
            'x':0.5,
            'xanchor': 'center',
            'yanchor': 'top'
        },
        xaxis_title='–î–∞—Ç–∞',
        yaxis_title='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤',
        legend_title='URL',
        xaxis=dict(
            tickformat='%Y-%m-%d',
            tickangle=45
        ),
        yaxis=dict(
            tickmode='linear',
            dtick=1  # –ü–æ–∫–∞–∑—É—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ü—ñ–ª—ñ —á–∏—Å–ª–∞
        ),
        hovermode='x unified',
        annotations=[
            dict(
                xref='paper',
                yref='paper',
                x=0,
                y=-0.2,
                showarrow=False,
                text="–¶–µ–π –≥—Ä–∞—Ñ—ñ–∫ –ø–æ–∫–∞–∑—É—î, —è–∫ –∑–º—ñ–Ω—é—î—Ç—å—Å—è –∑–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞ –∑ —á–∞—Å–æ–º.",
                font=dict(size=12)
            )

        ]
    )

    st.plotly_chart(fig, use_container_width=True)


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ —ñ—Å—Ç–æ—Ä–∏—á–Ω–æ–≥–æ –≥—Ä–∞—Ñ—ñ–∫–∞ –ø–æ –∫–ª—é—á–æ–≤–æ–º—É —Å–ª–æ–≤—É
def plot_keyword_history(df, keyword, selected_url, chart_type):
    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥–∞–Ω—ñ –ø–æ –æ–±—Ä–∞–Ω–æ–º—É URL
    url_data = df[df['url'] == selected_url].copy()
    if url_data.empty:
        st.error(f"–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è URL: {selected_url}")
        return

    # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞—Ç –Ω–∞ datetime
    url_data['date_checked'] = pd.to_datetime(url_data['date_checked'], errors='coerce')

    # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ª–æ–Ω–∫—É –∑ –∫—ñ–ª—å–∫—ñ—Å—Ç—é –ø–æ–≤—Ç–æ—Ä–µ–Ω—å –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞
    url_data['keyword_count'] = url_data['keywords_found'].apply(
        lambda row: extract_keywords(row).get(keyword.lower(), 0) if pd.notna(row) else 0
    )

    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥–∞–Ω—ñ, –∑–∞–ª–∏—à–∞—é—á–∏ –ª–∏—à–µ —Ç—ñ —Ä—è–¥–∫–∏, –¥–µ –∫–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ –ø—Ä–∏—Å—É—Ç–Ω—î
    url_data = url_data[url_data['keyword_count'] > 0]

    # –Ø–∫—â–æ –ø—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö –Ω–µ–º–∞—î, –ø–æ–≤—ñ–¥–æ–º–ª—è—î–º–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    if url_data.empty:
        st.warning(f"–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞ '{keyword}' –Ω–∞ –æ–±—Ä–∞–Ω–æ–º—É URL.")
        return

    # –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞
    fig = go.Figure()

    if chart_type == 'Line Chart':
        fig.add_trace(go.Scatter(
            x=url_data['date_checked'],
            y=url_data['keyword_count'],
            mode='lines+markers',
            name=selected_url,
            hoverinfo='text',
            hovertext=[
                f"–î–∞—Ç–∞: {date.strftime('%Y-%m-%d')}<br>–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–µ–Ω—å: {count}"
                for date, count in zip(url_data['date_checked'], url_data['keyword_count'])
            ]
        ))
    elif chart_type == 'Bar Chart':
        fig.add_trace(go.Bar(
            x=url_data['date_checked'],
            y=url_data['keyword_count'],
            name=selected_url,
            text=url_data['keyword_count'],
            textposition='auto',
            hoverinfo='text',
            hovertext=[
                f"–î–∞—Ç–∞: {date.strftime('%Y-%m-%d')}<br>–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–µ–Ω—å: {count}"
                for date, count in zip(url_data['date_checked'], url_data['keyword_count'])
            ]
        ))

    # –î–æ–¥–∞—î–º–æ –ø–æ—è—Å–Ω–µ–Ω–Ω—è –¥–æ –≥—Ä–∞—Ñ—ñ–∫–∞
    fig.update_layout(
        title=f'–Ü—Å—Ç–æ—Ä–∏—á–Ω–∏–π —Ç—Ä–µ–Ω–¥ –¥–ª—è –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞: {keyword}',
        xaxis_title='–î–∞—Ç–∞',
        yaxis_title='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–µ–Ω—å',
        xaxis=dict(tickformat='%Y-%m-%d', tickangle=45),
        annotations=[
            dict(
                xref='paper',
                yref='paper',
                x=0,
                y=-0.2,
                showarrow=False,
                text="–¶–µ–π –≥—Ä–∞—Ñ—ñ–∫ –ø–æ–∫–∞–∑—É—î, —è–∫ –∑–º—ñ–Ω—é–≤–∞–ª–∞—Å—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—å –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ –∑ —á–∞—Å–æ–º.",
                font=dict(size=12)
            )
        ]
    )

    st.plotly_chart(fig, use_container_width=True)


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≥—Ä–∞—Ñ—ñ–∫–∞ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤
def plot_comparison(df_list, competitor_names, selected_urls):
    fig = go.Figure()

    # –ü—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –≤—Å—ñ–º –æ–±—Ä–∞–Ω–∏–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º —Ç–∞ —ó—Ö —Å—Ç–æ—Ä—ñ–Ω–∫–∞–º
    for df, competitor, url in zip(df_list, competitor_names, selected_urls):
        url_data = df[df['url'] == url]
        if not url_data.empty:
            fig.add_trace(go.Scatter(x=url_data['date_checked'], y=url_data['keywords_count'],
                                     mode='lines', name=f'{competitor}: {url}'))
        else:
            st.write(f"No data for {competitor}: {url}")

    fig.update_layout(
        title='Keyword Count Comparison',
        xaxis_title='Date',
        yaxis_title='Keyword Count',
        xaxis=dict(tickformat='%Y-%m-%d', tickangle=45),
        legend_title="Competitor: URL"
    )

    st.plotly_chart(fig)

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –ø—ñ–¥—Å–≤—ñ—á—É–≤–∞–Ω–Ω—è–º –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
def highlight_keywords(text, keywords):
    if not text:
        return "No content found."
    for keyword in keywords:
        escaped_keyword = re.escape(keyword)  # –ó–∞—Ö–∏—Å—Ç –≤—ñ–¥ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤
        text = re.sub(f'({escaped_keyword})', r'<span style="color:red; font-weight:bold;">\1</span>', text, flags=re.IGNORECASE)
    return text

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥—É —Å—ñ—Ç–∫–∏ –∑–º—ñ–Ω –∑–∞ –º—ñ—Å—è—Ü—è–º–∏
def render_contribution_chart_by_months(change_dates, selected_year, conn, competitor, selected_page=None):
    st.markdown(
        """
        <style>
        .contribution-box {
            width: 12px;
            height: 12px;
            margin: 2px;
            display: inline-block;
            background-color: #ebedf0;
        }
        .contribution-level-1 { background-color: #c6e48b; }
        .contribution-level-2 { background-color: #7bc96f; }
        .contribution-level-3 { background-color: #239a3b; }
        .contribution-level-4 { background-color: #196127; }
        .contribution-box-container {
            display: flex;
            justify-content: space-around;
            flex-wrap: wrap;
            max-width: 900px;  /* –û–±–º–µ–∂—É—î–º–æ —à–∏—Ä–∏–Ω—É */
        }
        .month-column {
            display: grid;
            grid-template-rows: repeat(7, 14px);  /* 7 –¥–Ω—ñ–≤ –Ω–∞ —Ä—è–¥–æ–∫ */
            grid-auto-flow: column;  /* –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ø–æ –∫–æ–ª–æ–Ω–∫–∞—Ö */
            gap: 4px;
            margin-right: 20px;
            text-align: center;
        }
        .month-title {
            text-align: center;
            margin-bottom: 10px;
            font-weight: bold;
            font-size: 12px;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥–∞—Ç–∏ –∑–∞ –æ–±—Ä–∞–Ω–∏–º —Ä–æ–∫–æ–º
    change_dates['change_date'] = pd.to_datetime(change_dates['change_date']).dt.date
    change_dates = change_dates[change_dates['change_date'].apply(lambda x: x.year) == selected_year]

    def render_month_labels():
        months = {
            'Jan': 31, 'Feb': 28, 'Mar': 31, 'Apr': 30, 'May': 31, 'Jun': 30,
            'Jul': 31, 'Aug': 31, 'Sep': 30, 'Oct': 31, 'Nov': 30, 'Dec': 31
        }

        months_html = '<div style="display: flex; flex-wrap: wrap; gap: 20px;">'

        for month, days in months.items():
            month_html = f'<div style="text-align: center;"><div style="margin-bottom: 5px;">{month}</div>'
            month_html += f'<div style="display: grid; grid-template-columns: repeat(7, 14px); grid-gap: 2px;">'

            for day in range(1, days + 1):
                date = datetime(selected_year, list(months.keys()).index(month) + 1, day).date()

                # –ó–º—ñ–Ω—é—î–º–æ –∑–∞–ø–∏—Ç –≤ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—ñ–¥ —Ç–æ–≥–æ, –≤–∏–±—Ä–∞–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É —á–∏ –≤—Å—ñ –∑–º—ñ–Ω–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞
                if selected_page:
                    query = f"""
                    SELECT change_date 
                    FROM content_changes 
                    WHERE competitor_name = '{competitor}'
                    AND url = '{selected_page}'
                    AND change_date::date = '{date}'
                    """
                    result = pd.read_sql(query, conn)
                    pages = result['change_date'].tolist() if not result.empty else []
                else:
                    query = f"""
                    SELECT url 
                    FROM content_changes 
                    WHERE competitor_name = '{competitor}'
                    AND change_date::date = '{date}'
                    """
                    pages = pd.read_sql(query, conn)['url'].tolist()

                # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ä—ñ–≤–µ–Ω—å –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∫–æ–ª—å–æ—Ä—É
                if not pages:
                    level = 'contribution-box'
                    tooltip = f"{date} - –Ω–µ–º–∞—î –∑–º—ñ–Ω"
                else:
                    if len(pages) <= 1:
                        level = 'contribution-box contribution-level-1'
                    elif len(pages) <= 3:
                        level = 'contribution-box contribution-level-2'
                    elif len(pages) <= 5:
                        level = 'contribution-box contribution-level-3'
                    else:
                        level = 'contribution-box contribution-level-4'

                    # –§–æ—Ä–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ —Å–ø–ª–∏–≤–∞—é—á–æ–º—É –≤—ñ–∫–Ω—ñ
                    if selected_page:
                        tooltip = f"{date} - –∑–º—ñ–Ω–∏ –Ω–∞ —Ü—ñ–π —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ"
                    else:
                        tooltip = f"{date} - –∑–º—ñ–Ω–∏:\n" + "\n".join(pages)

                month_html += f'<div class="{level}" title="{tooltip}"></div>'

            month_html += '</div>'
            months_html += f'{month_html}</div>'

        months_html += '</div>'
        return months_html

    # –í–∏–∫–ª–∏–∫–∞—î–º–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –º—ñ—Å—è—Ü—ñ–≤
    st.markdown(render_month_labels(), unsafe_allow_html=True)

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏
st.set_page_config(page_title="Change Tracker", page_icon="üîç")


def get_competitors_from_db(conn):
    query = """
    SELECT table_name
    FROM information_schema.tables
    WHERE table_name LIKE '%_com';
    """

    # –í–∏–∫–æ–Ω—É—î–º–æ –∑–∞–ø–∏—Ç —ñ –æ—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü—å-–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤
    competitor_tables = pd.read_sql(query, conn)['table_name'].tolist()
    return competitor_tables

# –ù–æ–≤–∏–π —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª –ø–æ—à—É–∫ –≤ –≥—É–≥–ª —Ç–∞ –¥–æ–¥–∞–≤–∞–Ω–Ω—è –ê–†–Ü ChatGPT
#
# –õ–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –∑–∞–ø–∏—Ç—ñ–≤ —ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
logging.basicConfig(filename='api_usage.log', level=logging.INFO)


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –∑ —Ç–∞–±–ª–∏—Ü—ñ "keywords"
def get_keywords(connection):
    try:
        cursor = connection.cursor()
        cursor.execute('SELECT keyword FROM keywords ORDER BY keyword ASC')
        keywords = [row[0] for row in cursor.fetchall()]
        cursor.close()
        return keywords
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤: {e}")
        return []

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–≥—É –¥–ª—è –æ–±—Ä–∞–Ω–æ–≥–æ –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞
def get_tag_for_keyword(connection, keyword):
    try:
        cursor = connection.cursor()
        cursor.execute('SELECT tag FROM keywords WHERE keyword = %s', (keyword,))
        result = cursor.fetchone()
        cursor.close()
        if result:
            return result[0]
        else:
            return None
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ —Ç–µ–≥—É –¥–ª—è –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞ '{keyword}': {e}")
        return None

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –∑–∞ —Ç–µ–≥–æ–º
def get_keywords_by_tag(connection, tag):
    try:
        cursor = connection.cursor()
        cursor.execute('SELECT keyword FROM keywords WHERE tag = %s', (tag,))
        keywords = [row[0] for row in cursor.fetchall()]
        cursor.close()
        return keywords
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –∑–∞ —Ç–µ–≥–æ–º '{tag}': {e}")
        return []

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API
def get_api_usage(connection, today_str):
    try:
        cursor = connection.cursor()
        cursor.execute('SELECT count FROM api_usage WHERE date = %s', (today_str,))
        result = cursor.fetchone()
        if result:
            cursor.close()
            return result[0]
        else:
            # –Ø–∫—â–æ –∑–∞–ø–∏—Å—É –Ω–µ–º–∞—î, —Å—Ç–≤–æ—Ä–∏—Ç–∏ –π–æ–≥–æ –∑ count=0
            cursor.execute('INSERT INTO api_usage (date, count) VALUES (%s, %s)', (today_str, 0))
            connection.commit()
            cursor.close()
            return 0
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API: {e}")
        return 0

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API
def update_api_usage(connection, today_str, increment=1):
    try:
        cursor = connection.cursor()
        cursor.execute('SELECT count FROM api_usage WHERE date = %s', (today_str,))
        result = cursor.fetchone()
        if result:
            new_count = result[0] + increment
            cursor.execute('UPDATE api_usage SET count = %s WHERE date = %s', (new_count, today_str))
        else:
            cursor.execute('INSERT INTO api_usage (date, count) VALUES (%s, %s)', (today_str, increment))
        connection.commit()
        cursor.close()
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–Ω–æ–≤–ª–µ–Ω–Ω—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API: {e}")

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API
def get_api_usage_history(connection):
    try:
        cursor = connection.cursor()
        cursor.execute('SELECT date, count FROM api_usage ORDER BY date ASC')
        data = cursor.fetchall()
        cursor.close()
        return data
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ —ñ—Å—Ç–æ—Ä—ñ—ó –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API: {e}")
        return []

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–æ—à—É–∫—É
def perform_search(query, api_key, cx, region=None):
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        'q': query,
        'key': api_key,
        'cx': cx,
        'num': 10  # –ú–∞–∫—Å–∏–º—É–º 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    }
    if region:
        params['gl'] = region  # –î–æ–¥–∞–≤–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ä–µ–≥—ñ–æ–Ω—É

    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        results = response.json()
        return results.get('items', [])
    except requests.exceptions.RequestException as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫—É: {e}")
        return []

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏
def fetch_page_content(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        st.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É {url}: {e}")
        return ""

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏
def analyze_page_content(html_content, related_keywords):
    soup = BeautifulSoup(html_content, 'html.parser')

    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ Title
    title = soup.title.string if soup.title and soup.title.string else ''

    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ –º–µ—Ç–∞—Ç–µ–≥—É Description
    meta_description = ''
    meta = soup.find('meta', attrs={'name': 'description'})
    if meta and meta.get('content'):
        meta_description = meta.get('content')

    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ H1, H2, H3
    headers_text = ''
    for header_tag in ['h1', 'h2', 'h3']:
        headers = soup.find_all(header_tag)
        headers_text += ' '.join([header.get_text(separator=' ', strip=True) for header in headers])

    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏
    body_text = soup.get_text(separator=' ', strip=True)

    # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –≤—Ö–æ–¥–∂–µ–Ω—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
    def count_occurrences(text, keyword):
        if not text:
            return 0
        return len(re.findall(r'\b' + re.escape(keyword.lower()) + r'\b', text.lower()))

    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ —É —Ä—ñ–∑–Ω–∏—Ö —Å–µ–∫—Ü—ñ—è—Ö
    counts_title = {kw: count_occurrences(title, kw) for kw in related_keywords if count_occurrences(title, kw) > 0}
    counts_description = {kw: count_occurrences(meta_description, kw) for kw in related_keywords if count_occurrences(meta_description, kw) > 0}
    counts_headers = {kw: count_occurrences(headers_text, kw) for kw in related_keywords if count_occurrences(headers_text, kw) > 0}
    counts_content = {kw: count_occurrences(body_text, kw) for kw in related_keywords if count_occurrences(body_text, kw) > 0}

    # –ü—ñ–¥—Å—É–º–∫–æ–≤–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
    total_keyword_count = sum(counts_title.values()) + sum(counts_description.values()) + sum(counts_headers.values()) + sum(counts_content.values())

    # –§–æ—Ä–º—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É
    analysis_results = {
        'title': title,
        'description': meta_description,
        'headers': headers_text,
        'body': body_text,
        'counts_title': counts_title,
        'counts_description': counts_description,
        'counts_headers': counts_headers,
        'counts_content': counts_content,
        'total_keywords': total_keyword_count
    }

    return analysis_results

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è session_state
if 'search_results' not in st.session_state:
    st.session_state['search_results'] = []
if 'related_keywords' not in st.session_state:
    st.session_state['related_keywords'] = []

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ç–∞–±–ª–∏—Ü—ñ —Ç–∞ —ó—ó —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è, —è–∫—â–æ –≤–æ–Ω–∞ –≤—ñ–¥—Å—É—Ç–Ω—è
def create_page_analysis_table_if_not_exists(connection):
    try:
        cursor = connection.cursor()
        cursor.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_name = 'page_analysis'
            );
        """)
        table_exists = cursor.fetchone()[0]

        if not table_exists:
            cursor.execute('''
                CREATE TABLE page_analysis (
                    id SERIAL PRIMARY KEY,
                    url TEXT NOT NULL,
                    title TEXT,
                    description TEXT,
                    headers TEXT,
                    body TEXT,
                    keyword_counts JSONB,
                    analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
            ''')
            connection.commit()
            st.success("–¢–∞–±–ª–∏—Ü—è 'page_analysis' —É—Å–ø—ñ—à–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–∞.")
        else:
            st.info("–¢–∞–±–ª–∏—Ü—è 'page_analysis' –≤–∂–µ —ñ—Å–Ω—É—î, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è.")
        cursor.close()
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –∞–±–æ –ø–µ—Ä–µ–≤—ñ—Ä—Ü—ñ —Ç–∞–±–ª–∏—Ü—ñ: {e}")

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö
def save_analysis_results_to_db(connection, url, analysis_results):
    try:
        cursor = connection.cursor()
        cursor.execute('''
            INSERT INTO page_analysis (url, title, description, headers, body, keyword_counts, analyzed_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
        ''', (
            url,
            analysis_results['title'],
            analysis_results['description'],
            analysis_results['headers'],
            analysis_results['body'],
            json.dumps({
                'counts_title': analysis_results['counts_title'],
                'counts_description': analysis_results['counts_description'],
                'counts_headers': analysis_results['counts_headers'],
                'counts_content': analysis_results['counts_content']
            }),
            datetime.now()
        ))
        connection.commit()
        cursor.close()
        st.success(f"–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {url} —É—Å–ø—ñ—à–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö.")
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É: {e}")


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—Å—ñ—Ö –∑–±–µ—Ä–µ–∂–µ–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
def get_all_saved_results_from_db(connection):
    try:
        cursor = connection.cursor()
        cursor.execute('SELECT title, description, headers, body, keyword_counts FROM page_analysis')
        results = cursor.fetchall()
        cursor.close()
        return results
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö: {e}")
        return []



# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É –¥–æ OpenAI API —á–µ—Ä–µ–∑ requests
def get_openai_response(api_key, model, prompt):
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    data = {
        "model": model,  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º–æ–¥–µ–ª—å, –≤–∏–±—Ä–∞–Ω—É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7
    }

    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=data)

    if response.status_code == 200:
        return response.json()
    else:
        return f"Error: {response.status_code}, {response.text}"

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –ø—Ä–æ–º–ø—Ç—É –¥–ª—è OpenAI API
def generate_api_prompt_for_single_page(user_page_results, competitor_pages, keyword_group):
    """
    –ì–µ–Ω–µ—Ä—É—î –ø—Ä–æ–º–ø—Ç –¥–ª—è OpenAI –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤.

    :param user_page_results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    :param competitor_pages: —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫
    :param keyword_group: –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å–µ–∫—Ü—ñ–π
    :return: –ø—Ä–æ–º–ø—Ç –¥–ª—è OpenAI
    """
    prompt = """
    Terms of reference for SEO optimization based on page analysis. Focus on the following aspects: 
    meta description, headings (H1, H2, H3), main content, and FAQ section. Use the following group of keywords 
    for each section to improve the user's page.
    """

    # –î–∞–Ω—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    title = user_page_results['title']
    description = user_page_results['description']
    headers = user_page_results['headers']
    total_keywords = user_page_results['total_keywords']

    # –§–æ—Ä–º—É—î–º–æ –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    prompt += f"""
    1. **Page Title**:
    {title}

    Use these keywords to optimize your title: {keyword_group['title']}.
    Make it more attractive to search engines by considering the keyword group.

    2. **Meta Description**:
    {description}

    Use these keywords to improve your meta description: {keyword_group['description']}.
    Make it more informative, taking into account the keywords, and add a more detailed description of the page.

    3. **H1, H2, H3 headings**:
    {headers}

    Use these keywords to optimize your headings: {keyword_group['headers']}.
    Consider adding keywords to your headers to increase relevance. Describe how they can be changed or improved 
    to increase search engine visibility.

    4. **FAQ**:
    {'FAQ not found' if 'faq' not in user_page_results else user_page_results['faq']}

    Use these keywords to optimize your FAQ block: {keyword_group.get('faq', 'No FAQ keywords provided')}.
    Suggest how the FAQ can be expanded or improved to cover more questions and answers related to the keywords.

    5. **Total number of keywords on the page**: {total_keywords}
    """

    # –í—Ä–∞—Ö–æ–≤—É—î–º–æ –ø–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏
    prompt += "\nBased on the competitor analysis, improve the user's page with the following insights:\n\n"

    # –ü—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –≤—Å—ñ—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–∫–∞—Ö —ñ –¥–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é
    for idx, competitor in enumerate(competitor_pages, 1):
        competitor_title = competitor['title']
        competitor_description = competitor['description']
        competitor_headers = competitor['headers']
        competitor_body = competitor['body'][:500]  # –ü–µ—Ä—à—ñ 500 —Å–∏–º–≤–æ–ª—ñ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        competitor_total_keywords = competitor['total_keywords']

        prompt += f"""
        **Competitor {idx}:**
        - Title: {competitor_title}
        - Meta Description: {competitor_description}
        - Headers: {competitor_headers}
        - Main Content: {competitor_body}...
        - Total Keywords: {competitor_total_keywords}
        \n"""

    prompt += "\nBased on this analysis, prepare a technical task for SEO optimization of the user's page. Use keywords in each section to achieve the best SEO results."

    return prompt

#
#


# –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö —É Streamlit
def main():
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏
    st.set_page_config(page_title="SEO —Ç–∞ –ê–Ω–∞–ª—ñ–∑ –ó–º—ñ–Ω –ö–æ–Ω—Ç–µ–Ω—Ç—É", page_icon="üîç", layout="wide")

    # –õ–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –∑–∞–ø–∏—Ç—ñ–≤ —ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
    logging.basicConfig(filename='api_usage.log', level=logging.INFO)

    # –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
    conn = connect_to_db()
    if conn is None:
        st.stop()

    # –ë—ñ—á–Ω–∞ –ø–∞–Ω–µ–ª—å –¥–ª—è –Ω–∞–≤—ñ–≥–∞—Ü—ñ—ó
    st.sidebar.title("–ù–∞–≤—ñ–≥–∞—Ü—ñ—è")
    pages = [
        "–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–º—ñ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç—É",
        "–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤",
        "–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –º—ñ–∂ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏",
        "–ö–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –ø—ñ–¥—Å–≤—ñ—á–µ–Ω–∏–º–∏ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏",
        "–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É",
        "Google Custom Search –ê–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä"
    ]
    page_selection = st.sidebar.radio("–û–±–µ—Ä—ñ—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫—É", pages)

    # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –≤–∏–±–æ—Ä—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏
    if page_selection == "–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–º—ñ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç—É":
        render_content_change_visualization(conn)
    elif page_selection == "–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤":
        render_keyword_count(conn)
    elif page_selection == "–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –º—ñ–∂ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏":
        render_keyword_comparison(conn)
    elif page_selection == "–ö–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –ø—ñ–¥—Å–≤—ñ—á–µ–Ω–∏–º–∏ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏":
        render_page_content_with_keywords(conn)
    elif page_selection == "–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É":
        render_content_comparison(conn)
    elif page_selection == "Google Custom Search –ê–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä":
        render_google_custom_search_analyzer(conn)

# –§—É–Ω–∫—Ü—ñ—ó –¥–ª—è –∫–æ–∂–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
def render_content_change_visualization(conn):
    st.title("–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–º—ñ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç—É")

    # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤
    with st.spinner('–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å–ø–∏—Å–∫—É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤...'):
        competitors = get_competitors_from_content_changes(conn)

    competitor = st.selectbox("–í–∏–±–µ—Ä—ñ—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞", competitors, key="content_competitor_selectbox")

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –≤–∏–±—Ä–∞–Ω–∏–π —á–µ–∫–±–æ–∫—Å
    view_all = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç–∏ –≤—Å—ñ –∑–º—ñ–Ω–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞", key="content_view_all_checkbox")

    if view_all:
        # –ü–æ–∫–∞–∑—É—î–º–æ –≤—Å—ñ –∑–º—ñ–Ω–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞
        with st.spinner('–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –ø—Ä–æ –∑–º—ñ–Ω–∏...'):
            query = "SELECT change_date FROM content_changes WHERE competitor_name = %s"
            df = pd.read_sql(query, conn, params=[competitor])

        if not df.empty:
            years = sorted(pd.to_datetime(df['change_date']).dt.year.unique())
            selected_year = st.selectbox("–û–±–µ—Ä—ñ—Ç—å —Ä—ñ–∫", years, key="year_selectbox")
            st.subheader(f"–ó–∞–≥–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–∏ –¥–ª—è {competitor} —É {selected_year} —Ä–æ—Ü—ñ")
            render_contribution_chart_by_months(df, selected_year, conn, competitor)
        else:
            st.info("–ù–µ–º–∞—î –∑–º—ñ–Ω –¥–ª—è —Ü—å–æ–≥–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞.")
    else:
        # –Ø–∫—â–æ –Ω–µ –≤–∏–±—Ä–∞–Ω–æ "–ü–æ–∫–∞–∑–∞—Ç–∏ –≤—Å—ñ –∑–º—ñ–Ω–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞", –≤–∏–±–∏—Ä–∞—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É
        with st.spinner('–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å–ø–∏—Å–∫—É —Å—Ç–æ—Ä—ñ–Ω–æ–∫...'):
            page_query = "SELECT DISTINCT url FROM content_changes WHERE competitor_name = %s"
            pages = pd.read_sql(page_query, conn, params=[competitor])['url'].tolist()

        if not pages:
            st.info("–ù–µ–º–∞—î –¥–æ—Å—Ç—É–ø–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –¥–ª—è —Ü—å–æ–≥–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞.")
        else:
            selected_page = st.selectbox("–í–∏–±–µ—Ä—ñ—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫—É", pages, key="content_page_selectbox")

            # –ü–æ–∫–∞–∑—É—î–º–æ –∑–º—ñ–Ω–∏ –¥–ª—è –≤–∏–±—Ä–∞–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            with st.spinner('–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –ø—Ä–æ –∑–º—ñ–Ω–∏...'):
                query = "SELECT change_date FROM content_changes WHERE competitor_name = %s AND url = %s"
                df = pd.read_sql(query, conn, params=[competitor, selected_page])

            if not df.empty:
                years = sorted(pd.to_datetime(df['change_date']).dt.year.unique())
                selected_year = st.selectbox("–û–±–µ—Ä—ñ—Ç—å —Ä—ñ–∫", years, key="year_selectbox")
                st.subheader(f"–ó–º—ñ–Ω–∏ –¥–ª—è {competitor} –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {selected_page} —É {selected_year} —Ä–æ—Ü—ñ")
                render_contribution_chart_by_months(df, selected_year, conn, competitor, selected_page)
            else:
                st.info("–ù–µ–º–∞—î –∑–º—ñ–Ω –¥–ª—è —Ü—ñ—î—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏.")


def render_keyword_count(conn):
    st.title("–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤")
    # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤ —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü—ñ—é get_competitors_from_db
    competitors = get_competitors_from_db(conn)

    competitor_name = st.selectbox("–í–∏–±–µ—Ä—ñ—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞", competitors, key="keyword_competitor_selectbox")

    with st.spinner('–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö...'):
        df = get_keyword_data(conn, competitor_name)

    if not df.empty:
        df['date_checked'] = pd.to_datetime(df['date_checked'])

        # –§—ñ–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–∞—Ö
        min_date = df['date_checked'].min().date()
        max_date = df['date_checked'].max().date()
        start_date = st.date_input('–ü–æ—á–∞—Ç–∫–æ–≤–∞ –¥–∞—Ç–∞', min_date, min_value=min_date, max_value=max_date, key="keyword_start_date")
        end_date = st.date_input('–ö—ñ–Ω—Ü–µ–≤–∞ –¥–∞—Ç–∞', max_date, min_value=min_date, max_value=max_date, key="keyword_end_date")

        if start_date > end_date:
            st.error('–ü–æ—á–∞—Ç–∫–æ–≤–∞ –¥–∞—Ç–∞ –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø—ñ–∑–Ω—ñ—à–µ –∫—ñ–Ω—Ü–µ–≤–æ—ó –¥–∞—Ç–∏.')
            return

        df = df[(df['date_checked'].dt.date >= start_date) & (df['date_checked'].dt.date <= end_date)]

        selected_urls = st.multiselect('–í–∏–±–µ—Ä—ñ—Ç—å URL', df['url'].unique(), key="keyword_url_multiselect")

        if selected_urls:
            df = df[df['url'].isin(selected_urls)]

            if df.empty:
                st.warning("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –≤–∏–±—Ä–∞–Ω–∏—Ö URL —Ç–∞ –¥–∞—Ç.")
                return

            st.subheader(f'–¢—Ä–µ–Ω–¥ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –¥–ª—è {competitor_name}')
            plot_keyword_trend(df, competitor_name)

            selected_url_for_keywords = st.selectbox('–í–∏–±–µ—Ä—ñ—Ç—å URL –¥–ª—è –ø–µ—Ä–µ–≥–ª—è–¥—É –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤',
                                                     df['url'].unique(), key="keyword_url_selectbox")

            if selected_url_for_keywords:
                selected_page_data = df[df['url'] == selected_url_for_keywords].iloc[0]
                if selected_page_data['keywords_found'] and isinstance(selected_page_data['keywords_found'], str):
                    keywords_dict = extract_keywords(selected_page_data['keywords_found'])

                    st.write(f"**–ó–Ω–∞–π–¥–µ–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –Ω–∞ {selected_url_for_keywords}:**")
                    st.write(keywords_dict)

                    selected_keywords = st.multiselect('–í–∏–±–µ—Ä—ñ—Ç—å –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —ñ—Å—Ç–æ—Ä—ñ—ó',
                                                       list(keywords_dict.keys()),
                                                       key="keyword_select_multiselect")


                    if selected_keywords:
                        chart_type = st.selectbox("–¢–∏–ø –≥—Ä–∞—Ñ—ñ–∫–∞",
                                                  ['Line Chart', 'Bar Chart'], key="keyword_chart_type_selectbox")
                        for keyword in selected_keywords:
                            st.subheader(f'–Ü—Å—Ç–æ—Ä—ñ—è –¥–ª—è –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞: {keyword}')
                            keyword_history_df = get_keyword_history(conn, competitor_name, keyword)
                            if not keyword_history_df.empty:
                                plot_keyword_history(keyword_history_df, keyword, selected_url_for_keywords,
                                                     chart_type)
                            else:
                                st.warning(f"–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞: {keyword}")
                else:
                    st.warning("–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –≤–∏–±—Ä–∞–Ω–æ–≥–æ URL.")
        else:
            st.warning("–ë—É–¥—å –ª–∞—Å–∫–∞, –æ–±–µ—Ä—ñ—Ç—å —Ö–æ—á–∞ –± –æ–¥–∏–Ω URL —É —Ñ—ñ–ª—å—Ç—Ä–∞—Ö.")
    else:
        st.warning("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –≤–∏–±—Ä–∞–Ω–æ–≥–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞.")


def render_keyword_comparison(conn):
    st.title("–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –º—ñ–∂ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏")
    # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤ —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü—ñ—é get_competitors_from_db
    competitors = get_competitors_from_db(conn)

    selected_competitors = st.multiselect("–í–∏–±–µ—Ä—ñ—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è", competitors,
                                          default=competitors[:2], key="comparison_competitors_multiselect")
    df_list = [get_keyword_data(conn, competitor) for competitor in selected_competitors]

    selected_urls_for_comparison = []
    for competitor, df in zip(selected_competitors, df_list):
        selected_url = st.selectbox(f'–í–∏–±–µ—Ä—ñ—Ç—å URL –¥–ª—è {competitor}', df['url'].unique(),
                                    key=f"comparison_url_selectbox_{competitor}")
        selected_urls_for_comparison.append(selected_url)

    if len(selected_urls_for_comparison) == len(selected_competitors):
        plot_comparison(df_list, selected_competitors, selected_urls_for_comparison)


def render_page_content_with_keywords(conn):
    st.title("–ö–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –ø—ñ–¥—Å–≤—ñ—á–µ–Ω–∏–º–∏ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏")
    # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤ —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü—ñ—é get_competitors_from_db
    competitors = get_competitors_from_db(conn)

    competitor_name_content = st.selectbox("–í–∏–±–µ—Ä—ñ—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–≥–ª—è–¥—É –∫–æ–Ω—Ç–µ–Ω—Ç—É", competitors,
                                           key="content_competitor_selectbox_2")

    df_content = get_keyword_data(conn, competitor_name_content)

    if not df_content.empty:
        # –í–∏–±—ñ—Ä URL –¥–ª—è –ø–µ—Ä–µ–≥–ª—è–¥—É –∫–æ–Ω—Ç–µ–Ω—Ç—É
        selected_url_for_content = st.selectbox('–í–∏–±–µ—Ä—ñ—Ç—å URL –¥–ª—è –ø–µ—Ä–µ–≥–ª—è–¥—É –∫–æ–Ω—Ç–µ–Ω—Ç—É',
                                                df_content['url'].unique(), key="content_url_selectbox_2")

        # –í–∏–±—ñ—Ä –¥–∞—Ç–∏
        selected_date_for_content = st.selectbox('–í–∏–±–µ—Ä—ñ—Ç—å –¥–∞—Ç—É',
                                                 df_content[df_content['url'] == selected_url_for_content][
                                                     'date_checked'].dt.date.unique(),
                                                 key="content_date_selectbox")

        if selected_date_for_content:
            # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö –∑–∞ URL —Ç–∞ –¥–∞—Ç–æ—é
            page_content_data = df_content[(df_content['url'] == selected_url_for_content) & (
                    df_content['date_checked'].dt.date == selected_date_for_content)]

            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            page_content = page_content_data['content'].values[0]
            keywords_found = page_content_data['keywords_found'].values[0]

            # –û–±—Ä–æ–±–∫–∞ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
            keywords_dict = extract_keywords(keywords_found)
            highlighted_content = highlight_keywords(page_content, list(keywords_dict.keys()))

            # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É –∑ –ø—ñ–¥—Å–≤—ñ—á–µ–Ω–∏–º–∏ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
            st.markdown(f"<div style='white-space: pre-wrap; padding: 15px;'>{highlighted_content}</div>",
                        unsafe_allow_html=True)


def render_content_comparison(conn):
    st.title("–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É")
    # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤ —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü—ñ—é get_competitors_from_db
    competitors = get_competitors_from_db(conn)

    selected_competitor = st.selectbox('–í–∏–±–µ—Ä—ñ—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞', competitors)

    if selected_competitor:
        pages = get_pages_for_competitor(conn, selected_competitor)
        selected_page = st.selectbox('–í–∏–±–µ—Ä—ñ—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫—É', pages)

        if selected_page:
            # –í–∏–±—ñ—Ä –¥–≤–æ—Ö –¥–∞—Ç –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
            dates = get_dates_for_page(conn, selected_competitor, selected_page)

            if dates:
                # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç datetime –¥–ª—è –≤—ñ–¥–∂–µ—Ç–∞
                formatted_dates = [pd.to_datetime(date).date() for date in dates]

                # –í–∏–±—ñ—Ä –¥–∞—Ç —á–µ—Ä–µ–∑ –æ—Å–Ω–æ–≤–Ω—É –æ–±–ª–∞—Å—Ç—å
                selected_date1 = st.date_input('–í–∏–±–µ—Ä—ñ—Ç—å –ø–µ—Ä—à—É –¥–∞—Ç—É', min_value=min(formatted_dates),
                                               max_value=max(formatted_dates),
                                               value=min(formatted_dates),
                                               key="date1")
                selected_date2 = st.date_input('–í–∏–±–µ—Ä—ñ—Ç—å –¥—Ä—É–≥—É –¥–∞—Ç—É', min_value=min(formatted_dates),
                                               max_value=max(formatted_dates),
                                               value=max(formatted_dates),
                                               key="date2")

                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –≤–∏–±—Ä–∞–Ω–æ –¥–≤—ñ —Ä—ñ–∑–Ω—ñ –¥–∞—Ç–∏
                if selected_date1 and selected_date2 and selected_date1 != selected_date2:
                    # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–±—Ä–∞–Ω–∏—Ö –¥–∞—Ç —É —Å—Ç—Ä–æ–∫–∏ –¥–ª—è SQL-–∑–∞–ø–∏—Ç—É
                    date1_str = pd.to_datetime(
                        [date for date in dates if pd.to_datetime(date).date() == selected_date1][0])
                    date2_str = pd.to_datetime(
                        [date for date in dates if pd.to_datetime(date).date() == selected_date2][0])

                    # –û—Ç—Ä–∏–º—É—î–º–æ –¥–∞–Ω—ñ –¥–ª—è –æ–±—Ä–∞–Ω–∏—Ö –¥–∞—Ç
                    data1 = get_page_data(conn, selected_competitor, selected_page, date1_str)
                    data2 = get_page_data(conn, selected_competitor, selected_page, date2_str)

                    if not data1.empty and not data2.empty:

                        # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö (Title, H1, Description)
                        metadata_changes = [
                            {'–ü–æ–ª–µ': col, '–ë—É–ª–æ': data1[col].values[0], '–°—Ç–∞–ª–æ': data2[col].values[0]}
                            for col in ['title', 'h1', 'description'] if
                            data1[col].values[0] != data2[col].values[0]
                        ]

                        if metadata_changes:
                            st.subheader("–ó–º—ñ–Ω–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö:")
                            metadata_df = pd.DataFrame(metadata_changes)
                            visualize_metadata_changes(metadata_df)
                        else:
                            st.info("–ó–º—ñ–Ω —É –º–µ—Ç–∞–¥–∞–Ω–∏—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")

                        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –∑–º—ñ–Ω —É –∫–æ–Ω—Ç–µ–Ω—Ç—ñ
                        if data1['content'].values[0] != data2['content'].values[0]:
                            st.subheader("–ó–º—ñ–Ω–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç—ñ:")
                            visualize_content_changes(data1['content'].values[0], data2['content'].values[0])
                        else:
                            st.info("–ó–º—ñ–Ω —É –∫–æ–Ω—Ç–µ–Ω—Ç—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")

                        # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
                        if data1['keywords_found'].values[0] and data2['keywords_found'].values[0]:
                            keywords_comparison = compare_keywords(data1['keywords_found'].values[0],
                                                                   data2['keywords_found'].values[0])
                            if not keywords_comparison.empty:
                                st.subheader("–ó–º—ñ–Ω–∏ –≤ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª–æ–≤–∞—Ö:")
                                visualize_keywords_changes(keywords_comparison)
                            else:
                                st.info("–ó–º—ñ–Ω —É –∫–ª—é—á–æ–≤–∏—Ö —Å–ª–æ–≤–∞—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")

                        # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
                        if data1['keywords_count'].values[0] != data2['keywords_count'].values[0]:
                            st.subheader("–ó–º—ñ–Ω–∏ –≤ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤:")
                            st.table(pd.DataFrame({
                                '–ë—É–ª–æ': [data1['keywords_count'].values[0]],
                                '–°—Ç–∞–ª–æ': [data2['keywords_count'].values[0]]
                            }))
                        else:
                            st.info("–ó–º—ñ–Ω —É –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
                    else:
                        st.warning("–î–ª—è –æ–±—Ä–∞–Ω–∏—Ö –¥–∞—Ç –Ω–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è.")
                else:
                    st.warning("–û–±–µ—Ä—ñ—Ç—å –¥–≤—ñ —Ä—ñ–∑–Ω—ñ –¥–∞—Ç–∏ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è.")
            else:
                st.info("–ù–µ–º–∞—î –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –¥–∞—Ç –¥–ª—è —Ü—ñ—î—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏.")
# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ Google Custom Search –ê–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä
def render_google_custom_search_analyzer(conn):
    st.title("Google Custom Search –ê–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä")

    # –õ—ñ–≤–µ –º–µ–Ω—é –¥–ª—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
    with st.sidebar:
        st.header("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è API —Ç–∞ –ë–∞–∑–∏ –î–∞–Ω–∏—Ö")
        api_key = st.text_input("API –∫–ª—é—á Google", type="password")
        cx = st.text_input("Custom Search Engine ID (CX)")
        openai_api_key = st.text_input("OpenAI API Key", type="password")

        # –í–∏–±—ñ—Ä –º–æ–¥–µ–ª—ñ OpenAI
        st.subheader("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è OpenAI")
        models = ["gpt-4o-mini", "gpt-4o"]  # –î–æ–¥–∞–π—Ç–µ —ñ–Ω—à—ñ –º–æ–¥–µ–ª—ñ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏
        selected_model = st.selectbox("–û–±–µ—Ä—ñ—Ç—å –º–æ–¥–µ–ª—å OpenAI", options=models, index=0)

        st.markdown("---")

        # –í–∏–±—ñ—Ä —Ä–µ–≥—ñ–æ–Ω—É –ø–æ—à—É–∫—É
        regions = {
            "–°–®–ê": "us",
            "–í–µ–ª–∏–∫–∞ –ë—Ä–∏—Ç–∞–Ω—ñ—è": "uk",
            "–ö–∞–Ω–∞–¥–∞": "ca",
            "–ê–≤—Å—Ç—Ä–∞–ª—ñ—è": "au",
            "–ù—ñ–º–µ—á—á–∏–Ω–∞": "de",
            "–§—Ä–∞–Ω—Ü—ñ—è": "fr",
            "–Ü—Å–ø–∞–Ω—ñ—è": "es",
            "–Ü—Ç–∞–ª—ñ—è": "it",
            "–£–∫—Ä–∞—ó–Ω–∞": "ua",
        }
        selected_region = st.selectbox("–û–±–µ—Ä—ñ—Ç—å —Ä–µ–≥—ñ–æ–Ω –ø–æ—à—É–∫—É", options=list(regions.keys()), index=0)
        region_code = regions[selected_region]
        st.markdown("---")

    if not api_key or not cx or not openai_api_key:
        st.warning("–ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å –≤–∞—à—ñ API –∫–ª—é—á—ñ —Ç–∞ CX —É –±—ñ—á–Ω–æ–º—É –º–µ–Ω—é.")
        st.stop()

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—ñ, —è–∫—â–æ –≤–æ–Ω–∞ –Ω–µ —ñ—Å–Ω—É—î
    create_page_analysis_table_if_not_exists(conn)

    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ—Ç–æ—á–Ω–æ—ó –¥–∞—Ç–∏
    today_str = datetime.now().strftime("%Y-%m-%d")

    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API
    current_usage = get_api_usage(conn, today_str)
    api_limit = 100

    # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API
    col1, col2 = st.columns([1, 2])
    with col1:
        st.metric("–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ API-–∑–∞–ø–∏—Ç—ñ–≤ —Å—å–æ–≥–æ–¥–Ω—ñ", f"{current_usage}/{api_limit}")

    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
    keywords = get_keywords(conn)

    # –î–æ–¥–∞–º–æ –∑–º—ñ–Ω–Ω—É –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –≤–≤–µ–¥–µ–Ω–æ–≥–æ –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞
    manual_search = False

    # –ü–æ–ª–µ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω—è –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –≤—ñ–¥ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    st.subheader("–í–≤–µ–¥—ñ—Ç—å —Å–≤–æ—î –∫–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ –¥–ª—è –ø–æ—à—É–∫—É")
    user_keyword = st.text_input("–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ", "")

    if st.button("–í–∏–∫–æ–Ω–∞—Ç–∏ –ø–æ—à—É–∫ –∑–∞ –≤–≤–µ–¥–µ–Ω–∏–º –∫–ª—é—á–æ–≤–∏–º —Å–ª–æ–≤–æ–º"):
        manual_search = True  # –í–∏–∫–æ–Ω–∞–Ω–∏–π —Ä—É—á–Ω–∏–π –ø–æ—à—É–∫
        if current_usage >= api_limit:
            st.error("–í–∏ –¥–æ—Å—è–≥–ª–∏ –ª—ñ–º—ñ—Ç—É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API –Ω–∞ —Å—å–æ–≥–æ–¥–Ω—ñ. –°–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–≤—Ç—Ä–∞.")
            st.stop()

        if not user_keyword:
            st.warning("–í–≤–µ–¥—ñ—Ç—å –∫–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ –¥–ª—è –ø–æ—à—É–∫—É.")
            st.stop()

        # –í–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–æ—à—É–∫—É –∑–∞ –≤–≤–µ–¥–µ–Ω–∏–º –∫–ª—é—á–æ–≤–∏–º —Å–ª–æ–≤–æ–º
        search_results = perform_search(user_keyword, api_key, cx, region=region_code)
        if not search_results:
            st.info("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–∞ —Ü–∏–º –∫–ª—é—á–æ–≤–∏–º —Å–ª–æ–≤–æ–º.")
            st.stop()

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É session_state
        st.session_state['search_results'] = search_results

        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API
        update_api_usage(conn, today_str, increment=1)
        current_usage += 1

        # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫—É
        results_data = []
        for idx, item in enumerate(search_results, 1):
            url = item.get('link')
            title = item.get('title') if item.get('title') else ''
            snippet = item.get('snippet') if item.get('snippet') else ''
            results_data.append({
                '‚Ññ': idx,
                '–ù–∞–∑–≤–∞': title,
                '–ü–æ—Å–∏–ª–∞–Ω–Ω—è': url,
                '–û–ø–∏—Å': snippet,
            })

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É session_state
        st.session_state['search_results'] = results_data

    if not keywords:
        st.info("–¢–∞–±–ª–∏—Ü—è 'keywords' –ø–æ—Ä–æ–∂–Ω—è. –î–æ–¥–∞–π—Ç–µ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö.")
        conn.close()
        st.stop()

    # –í–∏–±—ñ—Ä –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞
    selected_keyword = st.selectbox("–û–±–µ—Ä—ñ—Ç—å –∫–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ –¥–ª—è –ø–æ—à—É–∫—É", keywords)

    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫—É –ø–æ—à—É–∫—É –∑–∞ –≤–∏–±—Ä–∞–Ω–∏–º –∫–ª—é—á–æ–≤–∏–º —Å–ª–æ–≤–æ–º
    if st.button("–í–∏–∫–æ–Ω–∞—Ç–∏ –ø–æ—à—É–∫"):
        if current_usage >= api_limit:
            st.error("–í–∏ –¥–æ—Å—è–≥–ª–∏ –ª—ñ–º—ñ—Ç—É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API –Ω–∞ —Å—å–æ–≥–æ–¥–Ω—ñ. –°–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–≤—Ç—Ä–∞.")
            st.stop()

        # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–≥—É –¥–ª—è –æ–±—Ä–∞–Ω–æ–≥–æ –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞
        tag = get_tag_for_keyword(conn, selected_keyword)
        if not tag:
            st.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ —Ç–µ–≥ –¥–ª—è –∫–ª—é—á–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞ '{selected_keyword}'.")
            st.stop()

        # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ–≤'—è–∑–∞–Ω–∏—Ö –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –∑–∞ —Ç–µ–≥–æ–º
        related_keywords = get_keywords_by_tag(conn, tag)
        if not related_keywords:
            st.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –ø–æ–≤'—è–∑–∞–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–µ–≥—É '{tag}'.")
            st.stop()

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è related_keywords —É session_state
        st.session_state['related_keywords'] = related_keywords

        with st.spinner('–í–∏–∫–æ–Ω—É—î—Ç—å—Å—è –ø–æ—à—É–∫...'):
            # –í–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–æ—à—É–∫—É
            search_results = perform_search(selected_keyword, api_key, cx, region=region_code)
            if not search_results:
                st.info("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–∞ —Ü–∏–º –∫–ª—é—á–æ–≤–∏–º —Å–ª–æ–≤–æ–º.")
                st.stop()

            # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API
            update_api_usage(conn, today_str, increment=1)
            current_usage += 1

        # –ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫—É
        results_data = []
        all_analysis_results = []
        for idx, item in enumerate(search_results, 1):
            url = item.get('link')
            title = item.get('title') if item.get('title') else ''
            snippet = item.get('snippet') if item.get('snippet') else ''

            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            html_content = fetch_page_content(url)
            if not html_content:
                analysis_results = {
                    'counts_title': {},
                    'counts_description': {},
                    'counts_headers': {},
                    'counts_content': {},
                    'total_keywords': 0
                }
            else:
                analysis_results = analyze_page_content(html_content, related_keywords)

            all_analysis_results.append(analysis_results)

            counts_title = analysis_results['counts_title']
            counts_description = analysis_results['counts_description']
            counts_headers = analysis_results['counts_headers']
            counts_content = analysis_results['counts_content']
            total_keywords = analysis_results['total_keywords']

            # –î–æ–¥–∞–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –¥–æ —Å–ø–∏—Å–∫—É
            results_data.append({
                '‚Ññ': idx,
                '–ù–∞–∑–≤–∞': title,
                '–ü–æ—Å–∏–ª–∞–Ω–Ω—è': url,
                '–û–ø–∏—Å': snippet,
                '–ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤ Title': sum(counts_title.values()),
                '–ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤ Description': sum(counts_description.values()),
                '–ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤ H1/H2/H3': sum(counts_headers.values()),
                '–ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤ Content': sum(counts_content.values()),
                '–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤': total_keywords
            })

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É session_state
        st.session_state['search_results'] = results_data

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ —î –ø–æ—à—É–∫–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤ session_state
    if 'search_results' in st.session_state:
        search_results = st.session_state['search_results']
    else:
        search_results = []

    # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫—É
    if st.session_state['search_results']:
        results_data = st.session_state['search_results']

        df_results = pd.DataFrame(results_data)
        # –ü–µ—Ä–µ–º—ñ—â–µ–Ω–Ω—è ‚Ññ –¥–æ –ø–µ—Ä—à–æ–≥–æ —Å—Ç–æ–≤–ø—Ü—è
        cols = df_results.columns.tolist()
        cols = [cols[0]] + sorted(cols[1:], key=lambda x: (x != '–ü–æ—Å–∏–ª–∞–Ω–Ω—è', x))
        df_results = df_results[cols]

        st.success(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(results_data)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤:")
        st.dataframe(df_results, use_container_width=True)

        # –î–æ–¥–∞–≤–∞–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –≤–∏–±–æ—Ä—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–≥–ª—è–¥—É
        if not manual_search:
            st.markdown("---")
            st.subheader("–î–µ—Ç–∞–ª—å–Ω–∏–π –ø–µ—Ä–µ–≥–ª—è–¥ —Å—Ç–æ—Ä—ñ–Ω–∫–∏")
            selected_result = st.selectbox("–û–±–µ—Ä—ñ—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫—É –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–≥–ª—è–¥—É", df_results['–ü–æ—Å–∏–ª–∞–Ω–Ω—è'])

            if selected_result:
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π –∑–∞–ø–∏—Å
                selected_record = next((item for item in results_data if item['–ü–æ—Å–∏–ª–∞–Ω–Ω—è'] == selected_result), None)
                if selected_record:
                    st.markdown(f"### {selected_record['–ù–∞–∑–≤–∞']}")
                    st.markdown(f"**–ü–æ—Å–∏–ª–∞–Ω–Ω—è:** [–ü–µ—Ä–µ–π—Ç–∏]({selected_result})")
                    st.markdown(f"**–û–ø–∏—Å:** {selected_record['–û–ø–∏—Å']}")
                    st.markdown("---")

                    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–∞ –∞–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏
                    html_content = fetch_page_content(selected_result)
                    if not html_content:
                        st.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏.")
                    else:
                        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ related_keywords —É session_state
                        if 'related_keywords' in st.session_state and st.session_state['related_keywords']:
                            related_keywords = st.session_state['related_keywords']
                            analysis_results = analyze_page_content(html_content, related_keywords)

                            counts_title = analysis_results['counts_title']
                            counts_description = analysis_results['counts_description']
                            counts_headers = analysis_results['counts_headers']
                            counts_content = analysis_results['counts_content']
                            total_keywords = analysis_results['total_keywords']

                            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—ñ –∑ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ —Ç–∞ —ó—Ö –∫—ñ–ª—å–∫—ñ—Å—Ç—é —É —Ä—ñ–∑–Ω–∏—Ö —Å–µ–∫—Ü—ñ—è—Ö
                            st.subheader("–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ")
                            data = {
                                '–°–µ–∫—Ü—ñ—è': [],
                                '–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ': [],
                                '–ö—ñ–ª—å–∫—ñ—Å—Ç—å': []
                            }
                            for kw, count in counts_title.items():
                                data['–°–µ–∫—Ü—ñ—è'].append('Title')
                                data['–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ'].append(kw)
                                data['–ö—ñ–ª—å–∫—ñ—Å—Ç—å'].append(count)
                            for kw, count in counts_description.items():
                                data['–°–µ–∫—Ü—ñ—è'].append('Description')
                                data['–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ'].append(kw)
                                data['–ö—ñ–ª—å–∫—ñ—Å—Ç—å'].append(count)
                            for kw, count in counts_headers.items():
                                data['–°–µ–∫—Ü—ñ—è'].append('H1/H2/H3')
                                data['–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ'].append(kw)
                                data['–ö—ñ–ª—å–∫—ñ—Å—Ç—å'].append(count)
                            for kw, count in counts_content.items():
                                data['–°–µ–∫—Ü—ñ—è'].append('Content')
                                data['–ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ'].append(kw)
                                data['–ö—ñ–ª—å–∫—ñ—Å—Ç—å'].append(count)

                            if any(data['–°–µ–∫—Ü—ñ—è']):
                                df_keyword_counts = pd.DataFrame(data)
                                st.table(df_keyword_counts)
                                st.markdown(f"**–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤:** {total_keywords}")
                            else:
                                st.info("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –Ω–∞ —Ü—ñ–π —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ.")
                        else:
                            st.error("–ü–æ–≤'—è–∑–∞–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –Ω–µ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ. –í–∏–∫–æ–Ω–∞–π—Ç–µ –ø–æ—à—É–∫ —Å–ø–æ—á–∞—Ç–∫—É.")
                            st.stop()

                # –î–æ–¥–∞–≤–∞–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –≤–≤–µ–¥–µ–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ URL –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
                st.subheader("–í–≤–µ–¥—ñ—Ç—å –æ—Å–Ω–æ–≤–Ω–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è")
                main_url = st.text_input("–û—Å–Ω–æ–≤–Ω–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è", "")

                if st.button("–ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–∞ –≤–∏–∫–æ–Ω–∞—Ç–∏ API –∑–∞–ø–∏—Ç"):
                    if not main_url:
                        st.error("–ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å –æ—Å–Ω–æ–≤–Ω–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è.")
                        st.stop()

                    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É –æ—Å–Ω–æ–≤–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
                    main_html_content = fetch_page_content(main_url)
                    if not main_html_content:
                        st.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {main_url}")
                    else:
                        # –ê–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É –æ—Å–Ω–æ–≤–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
                        related_keywords = st.session_state.get('related_keywords', [])
                        main_analysis_results = analyze_page_content(main_html_content, related_keywords)

                        # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏
                        competitor_pages = []
                        for result in search_results:
                            url = result.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è')
                            html_content = fetch_page_content(url)
                            if html_content:
                                analysis_results = analyze_page_content(html_content, related_keywords)
                                competitor_pages.append(analysis_results)

                        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—Ä–æ–º–ø—Ç—É –¥–ª—è OpenAI API
                        keyword_group = {
                            'title': related_keywords,
                            'description': related_keywords,
                            'headers': related_keywords,
                            'body': related_keywords,
                            'faq': related_keywords
                        }

                        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—Ä–æ–º–ø—Ç—É
                        prompt = generate_api_prompt_for_single_page(main_analysis_results, competitor_pages, keyword_group)

                        # –í–∏–∫–ª–∏–∫ —Ñ—É–Ω–∫—Ü—ñ—ó –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É –¥–æ OpenAI API
                        response = get_openai_response(openai_api_key, selected_model, prompt)

                        # –í–∏–≤–µ–¥–µ–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
                        st.subheader("–í—ñ–¥–ø–æ–≤—ñ–¥—å OpenAI:")
                        if isinstance(response, dict):
                            st.write(response.get('choices', [{}])[0].get('message', {}).get('content', '–ù–µ–º–∞—î –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ'))
                        else:
                            st.error(response)


if __name__ == "__main__":
    main()